{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# 生成模拟数据\n",
    "plot_theta = np.linspace(-2., 6., 50)\n",
    "plot_theta\n",
    "\n",
    "plot_J = (plot_theta - 2)**2 + 5\n",
    "plot_J\n",
    "\n",
    "# 绘图\n",
    "plt.figure()\n",
    "plt.plot(plot_theta, plot_J, color ='steelblue')\n",
    "plt.show()\n",
    "\n",
    "# 定义损失函数\n",
    "def Func_J(theta):\n",
    "    return  (theta - 2)**2 + 5\n",
    "\n",
    "# 定义求梯度函数\n",
    "def grad(theta):\n",
    "    return 2*(theta - 2)\n",
    "\n",
    "# 模拟以theta变化极小为收敛条件\n",
    "eta = 0.1 # 学习率\n",
    "epsilon = 1e-8 # 收敛阈值\n",
    "theta = 0.0 #初始theta\n",
    "i = 0\n",
    "while(True):\n",
    "    gradient = grad(theta)\n",
    "    theta_old = theta\n",
    "    theta = theta_old - eta * gradient\n",
    "    i += 1\n",
    "\n",
    "    if abs(theta - theta_old)<epsilon:\n",
    "        break\n",
    "\n",
    "print('一共迭代了{}次'.format(i))  \n",
    "print('以theta变化极小作为收敛方式时得到的参数theta为：{}'.format(theta))  \n",
    "print('以theta变化极小作为收敛方式时得到的最小值为：{}'.format(Func_J(theta)))\n",
    "\n",
    "# 以J(theta)变化极小作为收敛条件\n",
    "eta = 0.1 # 学习率\n",
    "epsilon = 1e-8 # 收敛阈值\n",
    "theta = 0.0 #初始theta\n",
    "i = 0\n",
    "while(True):\n",
    "    gradient = grad(theta)\n",
    "    theta_old = theta\n",
    "    theta = theta_old - eta * gradient\n",
    "    i += 1\n",
    "\n",
    "    if abs(Func_J(theta) - Func_J(theta_old))<epsilon:\n",
    "        break\n",
    "\n",
    "print('一共迭代了{}次'.format(i))  \n",
    "print('以J变化极小作为收敛方式时得到的参数theta为：{}'.format(theta))  \n",
    "print('以J变化极小作为收敛方式时得到的最小值为：{}'.format(Func_J(theta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#读取图像\n",
    "img = cv.imread(\"C:\\\\Users\\\\12541\\\\Desktop\\\\Lena(原始图像).bmp\", 0)\n",
    "\n",
    "#傅里叶变换\n",
    "f = np.fft.fft2(img)\n",
    "fshift = np.fft.fftshift(f)\n",
    "res = np.log(np.abs(fshift))\n",
    "\n",
    "#傅里叶逆变换\n",
    "ishift = np.fft.ifftshift(fshift)\n",
    "iimg = np.fft.ifft2(ishift)\n",
    "iimg = np.abs(iimg)\n",
    "\n",
    "#展示结果\n",
    "plt.subplot(131), plt.imshow(img, 'gray'), plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "plt.subplot(132), plt.imshow(res, 'gray'), plt.title('Fourier Image')\n",
    "plt.axis('off')\n",
    "plt.subplot(133), plt.imshow(iimg, 'gray'), plt.title('Inverse Fourier Image')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('./1_25_4.jpg')\n",
    "w, h = img.shape[0], img.shape[1]\n",
    "\n",
    "with open('./labels/1_25_4.txt') as f:\n",
    "    data = f.readlines()\n",
    "# print(data[0][0].type)\n",
    "num_obj = len(data)\n",
    "x = []; y = []; w1 = []; h1 = []\n",
    "for i in range(num_obj):\n",
    "    x.append(data[i].split()[1])\n",
    "    y.append(data[i].split()[2])\n",
    "    w1.append(data[i].split()[3])\n",
    "    h1.append(data[i].split()[4])\n",
    "# print(x)\n",
    "# print(data[0][1].split())\n",
    "# print(data[0].split()[3])\n",
    "# x *= w; y *= h; w1 *= w; h1 *= h\n",
    "x = [eval(a)*w for a in x]\n",
    "y = [eval(a)*h for a in y]\n",
    "w1 = [eval(a)*w for a in w1]\n",
    "h1= [eval(a)*h for a in h1]\n",
    "\n",
    "pt1_x = [x-w1//2 for x,w1 in zip(x, w1)]\n",
    "pt1_y = [y-h1//2 for y,h1 in zip(y, h1)]\n",
    "pt2_x = [x+w1//2 for x,w1 in zip(x, w1)]\n",
    "pt2_y = [y+h1//2 for y,h1 in zip(y, h1)]\n",
    "\n",
    "pt1_x = [np.round(pt1_x) for pt1_x in pt1_x]\n",
    "pt1_y = [np.round(pt1_y) for pt1_y in pt1_y]\n",
    "pt2_x = [np.round(pt2_x) for pt2_x in pt2_x]\n",
    "pt2_y = [np.round(pt2_y) for pt2_y in pt2_y]\n",
    "\n",
    "cls = 'car'\n",
    "# print((pt1_x[0],pt1_y[0]))\n",
    "img = cv2.rectangle(img, (int(pt1_x[0]), int(pt1_y[0])), (int(pt2_x[0]), int(pt2_y[0])), (0, 255, 0), 2)\n",
    "img = cv2.rectangle(img, (int(pt1_x[1]), int(pt1_y[1])), (int(pt2_x[1]), int(pt2_y[1])), (0, 255, 0), 2)\n",
    "# imgzi = cv2.putText(img, '{}'.format(cls), (pt1_x[0]+w1[0]//2, pt1_y[0]), fontFace=2, fontScale=2, color=(0,255,255))\n",
    "plt.imshow(img)\n",
    "# print(x)\n",
    "# plt.figure(figsize=(12,12))\n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A:\n",
    "    def __init__(self):\n",
    "        self.value = 0.1\n",
    "\n",
    "class B(A):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.value = 0.2\n",
    "\n",
    "class C(B):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.value = 0.3\n",
    "a = A()\n",
    "b = B()\n",
    "c = C()\n",
    "\n",
    "print(a.value)\n",
    "print(b.value)\n",
    "print(c.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_demo(x, y):\n",
    "    for _ in range(2):\n",
    "        x+=1\n",
    "        y+=1\n",
    "        result = x+y\n",
    "    return result\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    result = sum_demo(1,1)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "model = resnet18(weights = ResNet18_Weights.DEFAULT)\n",
    "data = torch.rand(1, 3, 64, 64)\n",
    "labels = torch.rand(1, 1000)\n",
    "\n",
    "prediction = model(data)\n",
    "loss = (prediction - labels).sum()\n",
    "loss.backward() # 反向传播开始\n",
    "\n",
    "optim = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "optim.step()\n",
    "\n",
    "print(loss.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n",
    "\n",
    "loss = 3*a**3 - b**2 # 可见此处的loss是一个vector\n",
    "external_grad = torch.tensor([1., 1.])\n",
    "loss.backward(gradient=external_grad)\n",
    "\n",
    "a_grad = a.grad\n",
    "b_grad = b.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square, you can specify with a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "\n",
    "input = torch.randn(1, 1, 32, 32)\n",
    "\n",
    "output = net(input)\n",
    "print(output.shape)\n",
    "target = torch.randn(10)\n",
    "target = target.view(1, -1)\n",
    "print(target.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([150, 150])\n",
      "torch.Size([3, 150, 150])\n",
      "torch.Size([3, 150, 150])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def generate_mask(img_height, img_width, radius, center_x, center_y):\n",
    "        y, x = np.ogrid[0:img_height, 0:img_width]\n",
    "\n",
    "        # circle mask\n",
    "\n",
    "        mask = (x - center_x) ** 2 + (y - center_y) ** 2 <= radius ** 2\n",
    "\n",
    "        return torch.from_numpy(mask)\n",
    "\n",
    "\n",
    "mask = generate_mask(150,150,50,50,50)\n",
    "img = torch.ones(150,150).expand(3,-1,-1)\n",
    "\n",
    "print(mask.shape)\n",
    "print(img.shape)\n",
    "adv_mask = torch.mul(mask, img)\n",
    "print(adv_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3.], requires_grad=True)\n",
      "tensor(6., grad_fn=<SumBackward0>)\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 2., 4., 6.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x1 = torch.arange(4.)\n",
    "x1.requires_grad_(True)\n",
    "y1 = x1.sum()\n",
    "y1.backward()\n",
    "x2 = torch.arange(4., requires_grad=True)\n",
    "y2 = x2**2\n",
    "y2.sum().backward()\n",
    "\n",
    "\n",
    "print(x1)\n",
    "print(y1)\n",
    "print(x1.grad)\n",
    "print(x2.grad)\n",
    "# print(y2.backward(torch.ones(len(x2)))==y2.sum().backward())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "\n",
    "class BaseConfig(object):\n",
    "    \"\"\"\n",
    "    Default parameters for all config files.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Set the defaults.\n",
    "        # \"\"\"\n",
    "        self.value = 300\n",
    "\n",
    "        self.start_learning_rate = 0.03\n",
    "\n",
    "        self.patch_name = 'base'\n",
    "        # 发现loss不再降低或acc不再上升时减小learning rate; x:optimizer\n",
    "        self.scheduler_factory = lambda x: optim.lr_scheduler.ReduceLROnPlateau(x, 'min', patience=50)\n",
    "        \n",
    "        self.max_tv = 0\n",
    "\n",
    "        self.batch_size = 4\n",
    "\n",
    "        self.loss_target = lambda obj, cls: obj * cls\n",
    "\n",
    "\n",
    "class Experiment1(BaseConfig):\n",
    "    \"\"\"\n",
    "    Model that uses a maximum total variation, tv cannot go below this point.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Change stuff...\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_name = 'Experiment1'\n",
    "        self.max_tv = 0.165\n",
    "\n",
    "\n",
    "class Experiment2HighRes(Experiment1):\n",
    "    \"\"\"\n",
    "    Higher res\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Change stuff...\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_tv = 0.165\n",
    "        self.patch_size = 400\n",
    "        self.patch_name = 'Exp2HighRes'\n",
    "\n",
    "class Experiment3LowRes(Experiment1):\n",
    "    \"\"\"\n",
    "    Lower res\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Change stuff...\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_tv = 0.165\n",
    "        self.patch_size = 200\n",
    "        self.patch_name = \"Exp3LowRes\"\n",
    "\n",
    "class Experiment4ClassOnly(Experiment1):\n",
    "    \"\"\"\n",
    "    Only minimise class score.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Change stuff...\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_name = 'Experiment4ClassOnly'\n",
    "        self.loss_target = lambda obj, cls: cls\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Experiment1Desktop(Experiment1):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Change batch size.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_size = 8\n",
    "        self.patch_size = 400\n",
    "\n",
    "\n",
    "class ReproducePaperObj(BaseConfig):\n",
    "    \"\"\"\n",
    "    Reproduce the results from the paper: Generate a patch that minimises object score.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_size = 3\n",
    "        self.patch_size = 150\n",
    "\n",
    "        self.patch_name = 'ObjectOnlyPaper'\n",
    "        self.max_tv = 0.165\n",
    "\n",
    "        self.loss_target = lambda obj, cls: obj\n",
    "        # self.loss_target = lambda obj, cls: 0.8 * obj + 0.2 * cls\n",
    "\n",
    "\n",
    "patch_configs = {\n",
    "    \"base\": BaseConfig,\n",
    "    \"exp1\": Experiment1,\n",
    "    \"exp1_des\": Experiment1Desktop,\n",
    "    \"exp2_high_res\": Experiment2HighRes,\n",
    "    \"exp3_low_res\": Experiment3LowRes,\n",
    "    \"exp4_class_only\": Experiment4ClassOnly,\n",
    "    \"paper_obj\": ReproducePaperObj\n",
    "}\n",
    "\n",
    "class patchtrainer():\n",
    "    def __init__(self, mode):\n",
    "        self.config = patch_configs[mode]()\n",
    "\n",
    "exp = patchtrainer('base')\n",
    "print(exp.config.value)\n",
    "# print(patch_configs['base']().value)\n",
    "# print(type(patch_configs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Example: 100%|██████████| 100/100 [00:10<00:00,  9.15it/s] \n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "from tqdm import tqdm\n",
    "\n",
    "iter = list(range(100))\n",
    "def action():\n",
    "    time.sleep(0.1) # 发呆0.5s\n",
    "    \n",
    "pbar = tqdm(iter, desc='Example')\n",
    "for i in pbar:\n",
    "    action()  \n",
    "    pbar.update(len(iter)/100)    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'region_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\12541\\Desktop\\exp\\Untitled-1.ipynb 单元格 13\u001b[0m in \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/12541/Desktop/exp/Untitled-1.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/12541/Desktop/exp/Untitled-1.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/12541/Desktop/exp/Untitled-1.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mload_datar\u001b[39;00m \u001b[39mimport\u001b[39;00m PatchTransformer, PatchApplier\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/12541/Desktop/exp/Untitled-1.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m patchfile \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mD:/Documents/bachelor/Graduation_Project/Project/train_patch/train_patch_a/saved_patches/v3/patchv3_2.jpg\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/12541/Desktop/exp/Untitled-1.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m weightfile \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mD:/Documents/bachelor/Graduation_Project/Project/train_patch/train_patch_a/weights/bestv3.pt\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\12541\\Desktop\\exp\\load_datar.py:18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m transforms\n\u001b[0;32m     17\u001b[0m \u001b[39m# from models.experimental import attempt_load\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdarknet\u001b[39;00m \u001b[39mimport\u001b[39;00m Darknet\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmedian_pool\u001b[39;00m \u001b[39mimport\u001b[39;00m MedianPool2d\n\u001b[0;32m     22\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mstarting test read\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\12541\\Desktop\\exp\\darknet.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mregion_loss\u001b[39;00m \u001b[39mimport\u001b[39;00m RegionLoss\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcfg\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mMaxPoolStride1\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'region_loss'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import numpy as np\n",
    "from load_datar import PatchTransformer, PatchApplier\n",
    "\n",
    "patchfile = \"D:/Documents/bachelor/Graduation_Project/Project/train_patch/train_patch_a/saved_patches/v3/patchv3_2.jpg\"\n",
    "weightfile = \"D:/Documents/bachelor/Graduation_Project/Project/train_patch/train_patch_a/weights/bestv3.pt\"\n",
    "imgdir = \"D:/Documents/bachelor/Graduation_Project/Project/train_patch/train_patch_a/kaiti-test-rotated-big\"\n",
    "savedir = \"D:/Documents/bachelor/Graduation_Project/Project/train_patch/train_patch_a/digital_test/\"\n",
    "img_size = 960\n",
    "patch_applier = PatchApplier.cuda()\n",
    "patch_transformer = PatchTransformer.cuda()\n",
    "# device = select_device('0')\n",
    "# model = attempt_load(weightfile, map_location=device)  # load FP32 model\n",
    "# stride = int(model.stride.max())  # model stride\n",
    "# img_size = check_img_size(img_size, s=stride)  # check img_size\n",
    "# patch_applier = PatchApplier().cuda()\n",
    "# patch_transformer = PatchTransformer().cuda()\n",
    "\n",
    "# batch_size = 1\n",
    "# max_lab = 80\n",
    "# img_size = darknet_model.height\n",
    "\n",
    "patch_size = 236\n",
    "\n",
    "patch_img = Image.open(patchfile).convert('RGB')\n",
    "tf = transforms.Resize((patch_size, patch_size))\n",
    "patch_img = tf(patch_img)\n",
    "tf = transforms.ToTensor()\n",
    "adv_patch_cpu = tf(patch_img)\n",
    "adv_patch = adv_patch_cpu.cuda()\n",
    "print(f'adv_patch.shape = {adv_patch.shape}')\n",
    "clean_results = []\n",
    "noise_results = []\n",
    "patch_results = []\n",
    "\n",
    "# padheight = (img_size - adv_patch.size(-1)) / 2\n",
    "# padwidth = (img_size - adv_patch.size(-2)) / 2\n",
    "# adv_patch = adv_patch.unsqueeze(0)\n",
    "# print(adv_patch.size())\n",
    "# print(adv_patch.size(-1))\n",
    "# print(adv_patch.size(-2))\n",
    "# print(adv_patch.size(-3))\n",
    "for imgfile in os.listdir(imgdir):\n",
    "    if imgfile.endswith('.jpg'):\n",
    "        name = os.path.splitext(imgfile)[0]\n",
    "        # print(name)\n",
    "        txtname = name + '.txt'\n",
    "        txtpath = os.path.abspath(os.path.join(savedir, 'clean/', 'yolo-labels/', txtname))\n",
    "        # print(txtname)\n",
    "        # print(txtpath)\n",
    "        imgfile = os.path.abspath(os.path.join(imgdir, imgfile))\n",
    "        # print(imgfile)\n",
    "        img = Image.open(imgfile).convert('RGB')\n",
    "        w, h =img.size\n",
    "        # print(img)\n",
    "        # img.show()\n",
    "        padding = (h - w)/2\n",
    "        padded_img = Image.new('RGB', (h,h), color=(127,127,127))\n",
    "        padded_img.paste(img, (int(padding), 0))\n",
    "        # print(padded_img)\n",
    "        resize = transforms.Resize((img_size, img_size))\n",
    "        padded_img = resize(padded_img)\n",
    "        # print(padded_img)\n",
    "        # padded_img.show()\n",
    "        padded_img1 = tf(padded_img)\n",
    "        padded_img1 = padded_img1.unsqueeze(0)\n",
    "        \n",
    "        transform = transforms.ToTensor()\n",
    "        padded_img = transform(padded_img).cuda()\n",
    "        print(padded_img.shape)\n",
    "        img_fake_batch = padded_img.unsqueeze(0)\n",
    "        print(f'img_fake_batch.shape = {img_fake_batch.shape}')\n",
    "        # print(padded_img1.shape)\n",
    "        if os.path.getsize(txtpath):\n",
    "            label = np.loadtxt(txtpath)\n",
    "        else:\n",
    "            label = np.ones([5])\n",
    "        label = torch.from_numpy(label).float()\n",
    "        print(f'label.shape = {label.shape}')\n",
    "        if label.dim()==1:\n",
    "            label = label.unsqueeze(0)\n",
    "        lab_fake_batch = label.unsqueeze(0).cuda()\n",
    "        print(f'lab_fake_batch = {lab_fake_batch}')\n",
    "        print(f'lab_fake_batch.shape = {lab_fake_batch.shape}')\n",
    "        \n",
    "        adv_batch_t = patch_transformer(adv_patch, lab_fake_batch, img_size, do_rotate=True, rand_loc=False)\n",
    "        p_img_batch = patch_applier(img_fake_batch, adv_batch_t)\n",
    "        break\n",
    "        \n",
    "# print(os.listdir(imgdir))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index of spring is 0\n",
      "The index of summer is 1\n",
      "The index of fall is 2\n",
      "The index of winter is 3\n"
     ]
    }
   ],
   "source": [
    "p = ['spring', 'summer', 'fall', 'winter']\n",
    "for index, cont in enumerate(p):\n",
    "    print(f'The index of {cont} is {index}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9*a**2 = tensor([36., 81.], grad_fn=<MulBackward0>), a.grad = tensor([36., 81.])\n",
      "-2*b = tensor([-12.,  -8.], grad_fn=<MulBackward0>), b.grad = tensor([-12.,  -8.])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\12541\\Desktop\\exp\\Untitled-1.ipynb 单元格 15\u001b[0m in \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/12541/Desktop/exp/Untitled-1.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m9*a**2 = \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m9\u001b[39m\u001b[39m*\u001b[39ma\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, a.grad = \u001b[39m\u001b[39m{\u001b[39;00ma\u001b[39m.\u001b[39mgrad\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/12541/Desktop/exp/Untitled-1.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m-2*b = \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mb\u001b[39m}\u001b[39;00m\u001b[39m, b.grad = \u001b[39m\u001b[39m{\u001b[39;00mb\u001b[39m.\u001b[39mgrad\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/12541/Desktop/exp/Untitled-1.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mloss.sum().backward()==loss.backward(gradient=external_grad) is \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mbackward()\u001b[39m==\u001b[39mloss\u001b[39m.\u001b[39mbackward(gradient\u001b[39m=\u001b[39mexternal_grad)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\12541\\AppData\\Local\\conda\\conda\\envs\\py38\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\12541\\AppData\\Local\\conda\\conda\\envs\\py38\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "import torch \n",
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n",
    "\n",
    "loss = 3*a**3 -b**2\n",
    "external_grad = torch.tensor([1., 1.])\n",
    "loss.sum().backward()\n",
    "print(f'9*a**2 = {9*a**2}, a.grad = {a.grad}')\n",
    "print(f'-2*b = {-2*b}, b.grad = {b.grad}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
